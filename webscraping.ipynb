{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulagambus/Documents/DTU/Fall_2024/social_graphs/sg_project/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# URL of Taylor Swift's reviews\n",
    "url = \"https://pitchfork.com/artists/28495-taylor-swift/review/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page fetched successfully!\n"
     ]
    }
   ],
   "source": [
    "# Fetch the webpage\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    print(\"Page fetched successfully!\")\n",
    "else:\n",
    "    print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "# Parse HTML\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete! Data saved to 'taylor_swift_reviews.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Extract review items\n",
    "reviews = soup.find_all('a', class_=\"SummaryItemHedLink-civMjp PNQqc summary-item-tracking__hed-link summary-item__hed-link\")\n",
    "data = []\n",
    "\n",
    "for review in reviews:\n",
    "    # Extract title and link\n",
    "    title_tag = review.find('h3', class_=\"SummaryItemHedBase-hiFYpQ jwYeiM summary-item__hed\")\n",
    "    link = review['href']\n",
    "    \n",
    "    # Some titles might be in <em> tags; handle gracefully\n",
    "    title = title_tag.text if title_tag else \"No Title\"\n",
    "    full_link = f\"https://pitchfork.com{link}\"\n",
    "\n",
    "    data.append({\"Title\": title, \"Link\": full_link})\n",
    "\n",
    "# Save to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Export to CSV\n",
    "df.to_csv(\"taylor_swift_reviews.csv\", index=False)\n",
    "\n",
    "print(\"Scraping complete! Data saved to 'taylor_swift_reviews.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV with review links\n",
    "input_csv = \"taylor_swift_reviews.csv\"  # Replace with your actual file name\n",
    "reviews_df = pd.read_csv(input_csv)\n",
    "\n",
    "# Ensure the \"Link\" and \"Title\" columns exist\n",
    "if \"Link\" not in reviews_df.columns or \"Title\" not in reviews_df.columns:\n",
    "    print(\"The CSV file must contain 'Link' and 'Title' columns!\")\n",
    "    exit()\n",
    "\n",
    "# Extract the titles and links\n",
    "review_titles = reviews_df[\"Title\"].tolist()\n",
    "review_links = reviews_df[\"Link\"].tolist()\n",
    "\n",
    "# List to store the scraped data\n",
    "review_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x1/r0lcq37s2gv_b_dbmtl4lrx40000gn/T/ipykernel_6004/1226610131.py:19: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  date_key = soup.find('p', text=\"Reviewed:\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped: The Tortured Poets Department / The Anthology\n",
      "Successfully scraped: 1989 (Taylor’s Version)\n",
      "Successfully scraped: Speak Now (Taylor’s Version)\n",
      "Successfully scraped: Midnights\n",
      "Successfully scraped: Red (Taylor’s Version)\n",
      "Successfully scraped: Fearless (Taylor’s Version)\n",
      "Successfully scraped: Evermore\n",
      "Successfully scraped: Folklore\n",
      "Successfully scraped: Lover\n",
      "Successfully scraped: Red\n",
      "Successfully scraped: Fearless\n",
      "Successfully scraped: Speak Now\n",
      "Successfully scraped: Taylor Swift\n",
      "Successfully scraped: 1989\n",
      "Successfully scraped: Reputation\n"
     ]
    }
   ],
   "source": [
    "# Scrape each review\n",
    "for idx, link in enumerate(review_links):\n",
    "    try:\n",
    "        response = requests.get(link)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {link}: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Title from the CSV\n",
    "        title = review_titles[idx]\n",
    "\n",
    "        # Extract the author\n",
    "        author_tag = soup.find('a', class_='byline__name-link')\n",
    "        author = author_tag.text.strip() if author_tag else \"No Author\"\n",
    "\n",
    "        # Extract the date (look for sibling after \"Reviewed:\")\n",
    "        date_key = soup.find('p', text=\"Reviewed:\")\n",
    "        date_tag = date_key.find_next_sibling('p') if date_key else None\n",
    "        date = date_tag.text.strip() if date_tag else \"No Date\"\n",
    "\n",
    "        # Extract the score\n",
    "        score_tag = soup.find('div', class_='ScoreCircle-jAxRuP')\n",
    "        score = score_tag.find('p').text.strip() if score_tag else \"No Score\"\n",
    "\n",
    "        # Extract the review text from multiple containers\n",
    "        review_text = \"\"\n",
    "\n",
    "        # Primary container\n",
    "        primary_container = soup.find('div', class_='body__inner-container')\n",
    "        if primary_container:\n",
    "            paragraphs = primary_container.find_all('p')\n",
    "            for p in paragraphs:\n",
    "                # Handle anchor tags in the paragraph\n",
    "                for a in p.find_all('a'):\n",
    "                    a.insert_before(' ')  # Add space before the <a> tag\n",
    "                    a.insert_after(' ')   # Add space after the <a> tag\n",
    "                    a.unwrap()  # Unwrap the anchor tag to get its text content\n",
    "                review_text += p.get_text(strip=True) + \" \"\n",
    "\n",
    "        # Additional container(s)\n",
    "        additional_containers = soup.find_all('div', class_='BodyWrapper-kufPGa')  # Adjust based on class\n",
    "        for container in additional_containers:\n",
    "            paragraphs = container.find_all('p')\n",
    "            for p in paragraphs:\n",
    "                # Handle anchor tags in the paragraph\n",
    "                for a in p.find_all('a'):\n",
    "                    a.insert_before(' ')  # Add space before the <a> tag\n",
    "                    a.insert_after(' ')   # Add space after the <a> tag\n",
    "                    a.unwrap()  # Unwrap the anchor tag to get its text content\n",
    "                review_text += p.get_text(strip=True) + \" \"\n",
    "                \n",
    "        # Append data to the list\n",
    "        review_data.append({\n",
    "            \"Title\": title,\n",
    "            \"Author\": author,\n",
    "            \"Date\": date,\n",
    "            \"Score\": score,\n",
    "            \"Text\": review_text,\n",
    "            \"Link\": link\n",
    "        })\n",
    "\n",
    "        print(f\"Successfully scraped: {title}\")\n",
    "\n",
    "        # Avoid hitting the server too quickly\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {link}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete! Data saved to 'taylor_swift_review_details.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Save the scraped data to a new CSV file\n",
    "output_csv = \"taylor_swift_review_details.csv\"\n",
    "df = pd.DataFrame(review_data)\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Scraping complete! Data saved to '{output_csv}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
